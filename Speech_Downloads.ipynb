{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDPEMmR4ysfx",
        "outputId": "b2dfc950-1460-4e7b-e726-6747b257c7fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2024.12.6-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading yt_dlp-2024.12.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=3748216f128f73da092606834b9b0ff07972ef52498a1bb4899239217a58865f\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: pydub, yt-dlp, triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 pydub-0.25.1 tiktoken-0.8.0 triton-3.1.0 yt-dlp-2024.12.6\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp pydub openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhopM9FxywST",
        "outputId": "a582f97c-5d1c-4f15-b841-b4d7e0978a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import yt_dlp\n",
        "from pydub import AudioSegment\n",
        "import whisper\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUzv4NjrTkeM",
        "outputId": "85439875-d0be-4a72-e7be-9184bb8141b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.exists(\"/content/drive/MyDrive\"))\n",
        "print(os.access(\"/content/drive/MyDrive\", os.W_OK))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N96E58GvJE1Q",
        "outputId": "f1caeabd-3752-46b2-ec3f-da355af6f3e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████| 1.51G/1.51G [00:16<00:00, 98.0MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"turbo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vk1tE7A9JD9Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define Google Drive paths\n",
        "drive_base_path = \"/content/drive/MyDrive/SpeechDownloads\"\n",
        "downloads_path = os.path.join(drive_base_path, \"downloads\")\n",
        "transcriptions_path = os.path.join(drive_base_path, \"transcriptions\")\n",
        "split_path = os.path.join(drive_base_path, \"split\")\n",
        "download_log = os.path.join(drive_base_path, \"download_log.json\")\n",
        "\n",
        "# Ensure output directories exist\n",
        "os.makedirs(downloads_path, exist_ok=True)\n",
        "os.makedirs(transcriptions_path, exist_ok=True)\n",
        "os.makedirs(split_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iHfOX5C_egCN"
      },
      "outputs": [],
      "source": [
        "def load_download_log():\n",
        "    \"\"\"Load the list of already downloaded files.\"\"\"\n",
        "    if os.path.exists(download_log):\n",
        "        with open(download_log, \"r\") as f:\n",
        "            return set(json.load(f))\n",
        "    return set()\n",
        "\n",
        "def save_download_log(downloaded_files):\n",
        "    \"\"\"Save the list of downloaded files.\"\"\"\n",
        "    with open(download_log, \"w\") as f:\n",
        "        json.dump(list(downloaded_files), f)\n",
        "\n",
        "def download_audio(query, num_videos=500, batch_size=100, retries=3):\n",
        "    \"\"\"Downloads audio from YouTube in batches with retry logic, filtering by video length.\"\"\"\n",
        "    downloaded_files = load_download_log()\n",
        "\n",
        "    for start in range(0, num_videos, batch_size):\n",
        "        batch_query = f\"{query} {start}..{start + batch_size - 1}\"\n",
        "        print(f\"Downloading batch {start + 1} to {start + batch_size}...\")\n",
        "\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3'}],\n",
        "            'default_search': f'ytsearch{batch_size}',\n",
        "            'postprocessor_args': [],\n",
        "            'noplaylist': False,\n",
        "            'outtmpl': os.path.join(downloads_path, '%(title)s.%(ext)s')\n",
        "        }\n",
        "\n",
        "        attempt = 0\n",
        "        while attempt < retries:\n",
        "            try:\n",
        "                with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                    entries = ydl.extract_info(batch_query, download=False)['entries']\n",
        "\n",
        "                    for query_result in entries:\n",
        "                        # Get video duration in seconds\n",
        "                        duration = query_result.get('duration', 0)\n",
        "\n",
        "                        # Skip videos that are less than 5 minutes or longer than 30 minutes\n",
        "                        if duration < 300 or duration > 1800:\n",
        "                            print(f\"Skipping video: {query_result['title']} (Duration: {duration}s)\")\n",
        "                            continue\n",
        "\n",
        "                        file_name = f\"{query_result['title']}.mp3\"\n",
        "                        if file_name in downloaded_files:\n",
        "                            print(f\"Skipping already downloaded file: {file_name}\")\n",
        "                            continue\n",
        "\n",
        "                        ydl.download([query_result['webpage_url']])\n",
        "                        downloaded_files.add(file_name)\n",
        "                        save_download_log(downloaded_files)\n",
        "\n",
        "                # Break out of the retry loop if successful\n",
        "                break\n",
        "\n",
        "            except yt_dlp.utils.DownloadError as e:\n",
        "                attempt += 1\n",
        "                print(f\"DownloadError occurred. Retrying... (Attempt {attempt}/{retries})\")\n",
        "                time.sleep(5)  # Wait before retrying\n",
        "                if attempt == retries:\n",
        "                    print(f\"Failed to download batch {start + 1} to {start + batch_size} after {retries} attempts.\")\n",
        "                    continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XwAicDK6eviJ"
      },
      "outputs": [],
      "source": [
        "download_audio(\"تعليم اللغة العربية\", num_videos=45, batch_size=2, retries=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O8wHXFcBf5GY"
      },
      "outputs": [],
      "source": [
        "download_audio(\"نشرات الأخبار باللغة العربي\", num_videos=45, batch_size=2, retries=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Pi3bJ8f5Wg",
        "outputId": "ef639e03-130b-4a23-cd6b-d2f8c8e960dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading batch 1 to 1...\n",
            "[generic] Extracting URL: روايات صوتية 0..0\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 0..0\n",
            "[download] Downloading playlist: روايات صوتية 0..0\n",
            "[youtube:search] query \"روايات صوتية 0..0\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 0..0\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 0..0: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=EbVmUb9s9vw\n",
            "[youtube] EbVmUb9s9vw: Downloading webpage\n",
            "[youtube] EbVmUb9s9vw: Downloading ios player API JSON\n",
            "[youtube] EbVmUb9s9vw: Downloading mweb player API JSON\n",
            "[youtube] EbVmUb9s9vw: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 0..0\n",
            "Skipping video: رواية مسموعة | الليالي البيضاء - دوستويفسكي (من أفضل أعماله) (Duration: 8645s)\n",
            "Downloading batch 2 to 2...\n",
            "[generic] Extracting URL: روايات صوتية 1..1\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 1..1\n",
            "[download] Downloading playlist: روايات صوتية 1..1\n",
            "[youtube:search] query \"روايات صوتية 1..1\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 1..1\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 1..1: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=zVYslgUGi6I\n",
            "[youtube] zVYslgUGi6I: Downloading webpage\n",
            "[youtube] zVYslgUGi6I: Downloading ios player API JSON\n",
            "[youtube] zVYslgUGi6I: Downloading mweb player API JSON\n",
            "[youtube] zVYslgUGi6I: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 1..1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=zVYslgUGi6I\n",
            "[youtube] zVYslgUGi6I: Downloading webpage\n",
            "[youtube] zVYslgUGi6I: Downloading ios player API JSON\n",
            "[youtube] zVYslgUGi6I: Downloading mweb player API JSON\n",
            "[youtube] zVYslgUGi6I: Downloading m3u8 information\n",
            "[info] zVYslgUGi6I: Downloading 1 format(s): 251\n",
            "[download] Destination: /content/drive/MyDrive/SpeechDownloads/downloads/الرحلة الى فضاء ارسس - رواية ارسس 1 - الجزء الاول.webm\n",
            "[download] 100% of   19.04MiB in 00:00:02 at 9.26MiB/s   \n",
            "[ExtractAudio] Destination: /content/drive/MyDrive/SpeechDownloads/downloads/الرحلة الى فضاء ارسس - رواية ارسس 1 - الجزء الاول.mp3\n",
            "Deleting original file /content/drive/MyDrive/SpeechDownloads/downloads/الرحلة الى فضاء ارسس - رواية ارسس 1 - الجزء الاول.webm (pass -k to keep)\n",
            "Downloading batch 3 to 3...\n",
            "[generic] Extracting URL: روايات صوتية 2..2\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 2..2\n",
            "[download] Downloading playlist: روايات صوتية 2..2\n",
            "[youtube:search] query \"روايات صوتية 2..2\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 2..2\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 2..2: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=RdqQA2e_AfE\n",
            "[youtube] RdqQA2e_AfE: Downloading webpage\n",
            "[youtube] RdqQA2e_AfE: Downloading ios player API JSON\n",
            "[youtube] RdqQA2e_AfE: Downloading mweb player API JSON\n",
            "[youtube] RdqQA2e_AfE: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 2..2\n",
            "Skipping video: صايع بالوراثة  | يوسف معاطي  | بصوت إسلام عادل (Duration: 14008s)\n",
            "Downloading batch 4 to 4...\n",
            "[generic] Extracting URL: روايات صوتية 3..3\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 3..3\n",
            "[download] Downloading playlist: روايات صوتية 3..3\n",
            "[youtube:search] query \"روايات صوتية 3..3\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 3..3\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 3..3: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=mCfYYUAQKn8\n",
            "[youtube] mCfYYUAQKn8: Downloading webpage\n",
            "[youtube] mCfYYUAQKn8: Downloading ios player API JSON\n",
            "[youtube] mCfYYUAQKn8: Downloading mweb player API JSON\n",
            "[youtube] mCfYYUAQKn8: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 3..3\n",
            "Skipping video: نساء في قطار الجاسوسية  | صالح مرسي | كتب صوتية روايات مسموعة بصوت إسلام عادل (Duration: 15127s)\n",
            "Downloading batch 5 to 5...\n",
            "[generic] Extracting URL: روايات صوتية 4..4\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 4..4\n",
            "[download] Downloading playlist: روايات صوتية 4..4\n",
            "[youtube:search] query \"روايات صوتية 4..4\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 4..4\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 4..4: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4LSkcMqsVxA\n",
            "[youtube] 4LSkcMqsVxA: Downloading webpage\n",
            "[youtube] 4LSkcMqsVxA: Downloading ios player API JSON\n",
            "[youtube] 4LSkcMqsVxA: Downloading mweb player API JSON\n",
            "[youtube] 4LSkcMqsVxA: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 4..4\n",
            "Skipping already downloaded file: اربع قصص قصيرة لـ تشيخوف | بصوت إسلام عادل.mp3\n",
            "Downloading batch 6 to 6...\n",
            "[generic] Extracting URL: روايات صوتية 5..5\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 5..5\n",
            "[download] Downloading playlist: روايات صوتية 5..5\n",
            "[youtube:search] query \"روايات صوتية 5..5\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 5..5\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 5..5: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/shorts/GR6AEXU1GGg\n",
            "[youtube] GR6AEXU1GGg: Downloading webpage\n",
            "[youtube] GR6AEXU1GGg: Downloading ios player API JSON\n",
            "[youtube] GR6AEXU1GGg: Downloading mweb player API JSON\n",
            "[youtube] GR6AEXU1GGg: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 5..5\n",
            "Skipping video: #كتب #روايات #like #subscribe #save #share #ارض_زيكولا (Duration: 20s)\n",
            "Downloading batch 7 to 7...\n",
            "[generic] Extracting URL: روايات صوتية 6..6\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 6..6\n",
            "[download] Downloading playlist: روايات صوتية 6..6\n",
            "[youtube:search] query \"روايات صوتية 6..6\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 6..6\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 6..6: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=dXIuBGcymRs\n",
            "[youtube] dXIuBGcymRs: Downloading webpage\n",
            "[youtube] dXIuBGcymRs: Downloading ios player API JSON\n",
            "[youtube] dXIuBGcymRs: Downloading mweb player API JSON\n",
            "[youtube] dXIuBGcymRs: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 6..6\n",
            "Skipping video: رواية مسموعة | العنبر رقم 6 - أنطون تشيخوف (أفضل ما كتب؟) (Duration: 10311s)\n",
            "Downloading batch 8 to 8...\n",
            "[generic] Extracting URL: روايات صوتية 7..7\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 7..7\n",
            "[download] Downloading playlist: روايات صوتية 7..7\n",
            "[youtube:search] query \"روايات صوتية 7..7\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 7..7\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 7..7: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=mCfYYUAQKn8\n",
            "[youtube] mCfYYUAQKn8: Downloading webpage\n",
            "[youtube] mCfYYUAQKn8: Downloading ios player API JSON\n",
            "[youtube] mCfYYUAQKn8: Downloading mweb player API JSON\n",
            "[youtube] mCfYYUAQKn8: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 7..7\n",
            "Skipping video: نساء في قطار الجاسوسية  | صالح مرسي | كتب صوتية روايات مسموعة بصوت إسلام عادل (Duration: 15127s)\n",
            "Downloading batch 9 to 9...\n",
            "[generic] Extracting URL: روايات صوتية 8..8\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 8..8\n",
            "[download] Downloading playlist: روايات صوتية 8..8\n",
            "[youtube:search] query \"روايات صوتية 8..8\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 8..8\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 8..8: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=mCfYYUAQKn8\n",
            "[youtube] mCfYYUAQKn8: Downloading webpage\n",
            "[youtube] mCfYYUAQKn8: Downloading ios player API JSON\n",
            "[youtube] mCfYYUAQKn8: Downloading mweb player API JSON\n",
            "[youtube] mCfYYUAQKn8: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 8..8\n",
            "Skipping video: نساء في قطار الجاسوسية  | صالح مرسي | كتب صوتية روايات مسموعة بصوت إسلام عادل (Duration: 15127s)\n",
            "Downloading batch 10 to 10...\n",
            "[generic] Extracting URL: روايات صوتية 9..9\n",
            "[youtube:search] Extracting URL: ytsearch1:روايات صوتية 9..9\n",
            "[download] Downloading playlist: روايات صوتية 9..9\n",
            "[youtube:search] query \"روايات صوتية 9..9\": Downloading web client config\n",
            "[youtube:search] query \"روايات صوتية 9..9\" page 1: Downloading API JSON\n",
            "[youtube:search] Playlist روايات صوتية 9..9: Downloading 1 items of 1\n",
            "[download] Downloading item 1 of 1\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Si47Il6l7-Q\n",
            "[youtube] Si47Il6l7-Q: Downloading webpage\n",
            "[youtube] Si47Il6l7-Q: Downloading ios player API JSON\n",
            "[youtube] Si47Il6l7-Q: Downloading mweb player API JSON\n",
            "[youtube] Si47Il6l7-Q: Downloading m3u8 information\n",
            "[download] Finished downloading playlist: روايات صوتية 9..9\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=Si47Il6l7-Q\n",
            "[youtube] Si47Il6l7-Q: Downloading webpage\n",
            "[youtube] Si47Il6l7-Q: Downloading ios player API JSON\n",
            "[youtube] Si47Il6l7-Q: Downloading mweb player API JSON\n",
            "[youtube] Si47Il6l7-Q: Downloading m3u8 information\n",
            "[info] Si47Il6l7-Q: Downloading 1 format(s): 251\n",
            "[download] Destination: /content/drive/MyDrive/SpeechDownloads/downloads/قصة في 9 رسائل ｜  فيودور دوستويفسكي ｜ روايات مسموعة ｜ كتب صوتية ｜ بصوت إسلام عادل.webm\n",
            "[download] 100% of   28.35MiB in 00:00:05 at 5.07MiB/s   \n",
            "[ExtractAudio] Destination: /content/drive/MyDrive/SpeechDownloads/downloads/قصة في 9 رسائل ｜  فيودور دوستويفسكي ｜ روايات مسموعة ｜ كتب صوتية ｜ بصوت إسلام عادل.mp3\n",
            "Deleting original file /content/drive/MyDrive/SpeechDownloads/downloads/قصة في 9 رسائل ｜  فيودور دوستويفسكي ｜ روايات مسموعة ｜ كتب صوتية ｜ بصوت إسلام عادل.webm (pass -k to keep)\n"
          ]
        }
      ],
      "source": [
        "download_audio(\"روايات صوتية\", num_videos=10, batch_size=1, retries=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OoLjrTftfBZ3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_audio(file_path, output_dir, segment_duration=15 * 60 * 1000):\n",
        "    \"\"\"Splits an MP3 audio file into 15-minute segments and saves them to a specified directory.\"\"\"\n",
        "    audio = AudioSegment.from_mp3(file_path)\n",
        "    segments = [audio[i:i + segment_duration] for i in range(0, len(audio), segment_duration)]\n",
        "    segment_paths = []\n",
        "\n",
        "    for idx, segment in enumerate(segments):\n",
        "        segment_name = f\"{os.path.splitext(os.path.basename(file_path))[0]}_part{idx + 1}.mp3\"\n",
        "        segment_path = os.path.join(output_dir, segment_name)\n",
        "        segment.export(segment_path, format=\"mp3\")\n",
        "        segment_paths.append(segment_path)\n",
        "\n",
        "    return segment_paths\n",
        "\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    \"\"\"Transcribes an MP3 file using Whisper's base model in Arabic.\"\"\"\n",
        "    result = model.transcribe(file_path, language=\"ar\")\n",
        "    return result[\"text\"]\n",
        "\n",
        "\n",
        "def transcribe_existing_videos():\n",
        "    \"\"\"Processes and transcribes all audio files already downloaded.\"\"\"\n",
        "    downloaded_files = load_download_log()\n",
        "\n",
        "    for file_name in os.listdir(downloads_path):\n",
        "        file_path = os.path.join(downloads_path, file_name)\n",
        "        if file_name not in downloaded_files:\n",
        "            print(f\"Processing {file_name}...\")\n",
        "\n",
        "            # Split audio into 15-minute segments\n",
        "            segments = split_audio(file_path, split_path)\n",
        "\n",
        "            # Transcribe each segment\n",
        "            for idx, segment_path in enumerate(segments):\n",
        "                transcription = transcribe_audio(segment_path)\n",
        "\n",
        "                # Save transcription to the transcriptions directory\n",
        "                transcription_file = os.path.join(\n",
        "                    transcriptions_path, f\"{file_name}_part{idx + 1}.txt\"\n",
        "                )\n",
        "                with open(transcription_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(transcription)\n",
        "\n",
        "                print(f\"Transcription for {file_name} part {idx + 1} saved.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(f\"Skipping already processed file: {file_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLf1bgw_fxoC",
        "outputId": "6636a3b7-8c55-4ed4-cdfa-de9b391e0e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing (15) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸01⧸19م.mp3...\n",
            "Transcription for (15) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸01⧸19م.mp3 part 1 saved.\n",
            "Processing (37) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸29م.mp3...\n",
            "Transcription for (37) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸29م.mp3 part 1 saved.\n",
            "Processing (17) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸02م.mp3...\n",
            "Transcription for (17) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸02م.mp3 part 1 saved.\n",
            "Processing (49) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸09⧸20م.mp3...\n",
            "Transcription for (49) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸09⧸20م.mp3 part 1 saved.\n",
            "Processing (19) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸16م.mp3...\n",
            "Transcription for (19) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸16م.mp3 part 1 saved.\n",
            "Processing (42) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸03م.mp3...\n",
            "Transcription for (42) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸03م.mp3 part 1 saved.\n",
            "Processing (41) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸27م.mp3...\n",
            "Transcription for (41) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸27م.mp3 part 1 saved.\n",
            "Processing (33) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸25م.mp3...\n",
            "Transcription for (33) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸25م.mp3 part 1 saved.\n",
            "Processing (39) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸13م.mp3...\n",
            "Transcription for (39) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸13م.mp3 part 1 saved.\n",
            "Processing (51) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸10⧸05م.mp3...\n",
            "Transcription for (51) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸10⧸05م.mp3 part 1 saved.\n",
            "Processing (36) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸22م.mp3...\n",
            "Transcription for (36) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸22م.mp3 part 1 saved.\n",
            "Processing (08) نشرة أخبار اللغة العربية الأسبوعية - 2017⧸12⧸01م.mp3...\n",
            "Transcription for (08) نشرة أخبار اللغة العربية الأسبوعية - 2017⧸12⧸01م.mp3 part 1 saved.\n",
            "Processing (40) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸20م.mp3...\n",
            "Transcription for (40) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸20م.mp3 part 1 saved.\n",
            "Processing (43) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸10م.mp3...\n",
            "Transcription for (43) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸10م.mp3 part 1 saved.\n",
            "Processing (44) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸17م.mp3...\n",
            "Transcription for (44) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸17م.mp3 part 1 saved.\n",
            "Processing (32) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸18م.mp3...\n",
            "Transcription for (32) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸18م.mp3 part 1 saved.\n",
            "Processing (38) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸06م.mp3...\n",
            "Transcription for (38) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸06م.mp3 part 1 saved.\n",
            "Processing (75) نشرة أخبار اللغة العربية - 2019⧸11⧸01 م.mp3...\n",
            "Transcription for (75) نشرة أخبار اللغة العربية - 2019⧸11⧸01 م.mp3 part 1 saved.\n",
            "Processing عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية.mp3...\n",
            "Transcription for عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية.mp3 part 1 saved.\n",
            "Transcription for عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية.mp3 part 2 saved.\n",
            "Processing (78) نشرة أخبار اللغة العربية - 2020⧸02⧸07 م.mp3...\n",
            "Transcription for (78) نشرة أخبار اللغة العربية - 2020⧸02⧸07 م.mp3 part 1 saved.\n",
            "Processing كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري.mp3...\n",
            "Transcription for كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري.mp3 part 1 saved.\n",
            "Transcription for كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري.mp3 part 2 saved.\n",
            "Processing الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر.mp3...\n",
            "Transcription for الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر.mp3 part 1 saved.\n",
            "Transcription for الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر.mp3 part 2 saved.\n",
            "Processing ARABIC LANGUAGE ⧸ DAILY RUTINE. Listening for Beginners..mp3...\n",
            "Transcription for ARABIC LANGUAGE ⧸ DAILY RUTINE. Listening for Beginners..mp3 part 1 saved.\n",
            "Processing تعليم العربية للناطقين بغيرها كيف تحسن لغتك العربية ｜ How to learn arabic.mp3...\n",
            "Transcription for تعليم العربية للناطقين بغيرها كيف تحسن لغتك العربية ｜ How to learn arabic.mp3 part 1 saved.\n",
            "Processing اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل.mp3...\n",
            "Transcription for اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل.mp3 part 1 saved.\n",
            "Transcription for اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل.mp3 part 2 saved.\n",
            "Processing طريق الفصاحة 1 (المستوى الأول) تعلم اللغة العربية للمبتدئين دراسة شاملة من العجمة إلى الفصاحة.mp3...\n",
            "Transcription for طريق الفصاحة 1 (المستوى الأول) تعلم اللغة العربية للمبتدئين دراسة شاملة من العجمة إلى الفصاحة.mp3 part 1 saved.\n",
            "Processing 1- تعليم القراءة في اللغة العربية الدرس الأول  Arabic  alphabet and how to read the Arabic language.mp3...\n",
            "Transcription for 1- تعليم القراءة في اللغة العربية الدرس الأول  Arabic  alphabet and how to read the Arabic language.mp3 part 1 saved.\n",
            "Processing كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس.mp3...\n",
            "Transcription for كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس.mp3 part 1 saved.\n",
            "Transcription for كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس.mp3 part 2 saved.\n",
            "Processing Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال.mp3...\n",
            "Transcription for Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال.mp3 part 1 saved.\n",
            "Transcription for Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال.mp3 part 2 saved.\n",
            "Processing نشرة أخبار الرياضة باللغة العربية 18⧸01⧸2013.mp3...\n",
            "Transcription for نشرة أخبار الرياضة باللغة العربية 18⧸01⧸2013.mp3 part 1 saved.\n",
            "Processing كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست.mp3...\n",
            "Transcription for كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست.mp3 part 1 saved.\n",
            "Transcription for كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست.mp3 part 2 saved.\n",
            "Processing (81) نشرة أخبار اللغة العربية - 2021⧸12⧸31م.mp3...\n",
            "Transcription for (81) نشرة أخبار اللغة العربية - 2021⧸12⧸31م.mp3 part 1 saved.\n",
            "Processing تعليم القراءة للاطفال ｜ تعلّم القراءة بحركة الفتح ｜ أسهل طريقة لتعليم القراءة للصغار مع زكريا.mp3...\n",
            "Transcription for تعليم القراءة للاطفال ｜ تعلّم القراءة بحركة الفتح ｜ أسهل طريقة لتعليم القراءة للصغار مع زكريا.mp3 part 1 saved.\n",
            "Skipping already processed file: كيف نتعلم الكتابة في ثلاث خطوات How to learn writing in three steps.mp3\n",
            "Processing (27) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸04⧸13م.mp3...\n",
            "Transcription for (27) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸04⧸13م.mp3 part 1 saved.\n",
            "Processing (63) نشرة أخبار اللغة العربية - 2019⧸03⧸21م.mp3...\n",
            "Transcription for (63) نشرة أخبار اللغة العربية - 2019⧸03⧸21م.mp3 part 1 saved.\n",
            "Processing تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين.mp3...\n",
            "Transcription for تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين.mp3 part 1 saved.\n",
            "Transcription for تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين.mp3 part 2 saved.\n",
            "Skipping already processed file: كيف تحسن لغتك العربية ؟.mp3\n",
            "Processing هل تعلم؟ ｜ البلدان العربية (الحلقة ٩)  - أسئلة و أجوبة عن الدول العربية – تعلم مع زكريا.mp3...\n",
            "Transcription for هل تعلم؟ ｜ البلدان العربية (الحلقة ٩)  - أسئلة و أجوبة عن الدول العربية – تعلم مع زكريا.mp3 part 1 saved.\n",
            "Processing 29.دورة تعليم القراءة في اللغة العربية Learn Arabic⧸alphabet⧸reading.mp3...\n",
            "Transcription for 29.دورة تعليم القراءة في اللغة العربية Learn Arabic⧸alphabet⧸reading.mp3 part 1 saved.\n",
            "Processing تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار ).mp3...\n",
            "Transcription for تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار ).mp3 part 1 saved.\n",
            "Transcription for تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار ).mp3 part 2 saved.\n",
            "Processing Arabic Conversation for Beginners ｜ 70 Basic Arabic Phrases To Know.mp3...\n",
            "Transcription for Arabic Conversation for Beginners ｜ 70 Basic Arabic Phrases To Know.mp3 part 1 saved.\n",
            "Processing علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي.mp3...\n",
            "Transcription for علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي.mp3 part 1 saved.\n",
            "Transcription for علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي.mp3 part 2 saved.\n",
            "Skipping already processed file: الرحلة الى فضاء ارسس - رواية ارسس 1 - الجزء الاول.mp3\n",
            "Processing قصة في 9 رسائل ｜  فيودور دوستويفسكي ｜ روايات مسموعة ｜ كتب صوتية ｜ بصوت إسلام عادل.mp3...\n",
            "Transcription for قصة في 9 رسائل ｜  فيودور دوستويفسكي ｜ روايات مسموعة ｜ كتب صوتية ｜ بصوت إسلام عادل.mp3 part 1 saved.\n",
            "Transcription for قصة في 9 رسائل ｜  فيودور دوستويفسكي ｜ روايات مسموعة ｜ كتب صوتية ｜ بصوت إسلام عادل.mp3 part 2 saved.\n"
          ]
        }
      ],
      "source": [
        "transcribe_existing_videos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "flvxdbNVnk80"
      },
      "outputs": [],
      "source": [
        "# Paths to directories\n",
        "audio_dir = \"/content/drive/MyDrive/SpeechDownloads/split\"\n",
        "transcriptions_dir = \"/content/drive/MyDrive/SpeechDownloads/processed/transcriptions\"\n",
        "output_dir = \"/content/drive/MyDrive/SpeechDownloads/processed\"\n",
        "os.makedirs(transcriptions_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OopPSU0Dtmbl",
        "outputId": "6756f426-f295-471a-9df1-a0c294cf1ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing audio files...\n",
            "Processing audio: تعليم العربية للناطقين بغيرها كيف تحسن لغتك العربية ｜ How to learn arabic_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/تعليم العربية للناطقين بغيرها كيف تحسن لغتك العربية ｜ How to learn arabic_part1.wav\n",
            "Processing audio: طريق الفصاحة 1 (المستوى الأول) تعلم اللغة العربية للمبتدئين دراسة شاملة من العجمة إلى الفصاحة_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/طريق الفصاحة 1 (المستوى الأول) تعلم اللغة العربية للمبتدئين دراسة شاملة من العجمة إلى الفصاحة_part1.wav\n",
            "Processing audio: تعليم القراءة للاطفال ｜ تعلّم القراءة بحركة الفتح ｜ أسهل طريقة لتعليم القراءة للصغار مع زكريا_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/تعليم القراءة للاطفال ｜ تعلّم القراءة بحركة الفتح ｜ أسهل طريقة لتعليم القراءة للصغار مع زكريا_part1.wav\n",
            "Processing audio: 1- تعليم القراءة في اللغة العربية الدرس الأول  Arabic  alphabet and how to read the Arabic language_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/1- تعليم القراءة في اللغة العربية الدرس الأول  Arabic  alphabet and how to read the Arabic language_part1.wav\n",
            "Processing audio: هل تعلم؟ ｜ البلدان العربية (الحلقة ٩)  - أسئلة و أجوبة عن الدول العربية – تعلم مع زكريا_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/هل تعلم؟ ｜ البلدان العربية (الحلقة ٩)  - أسئلة و أجوبة عن الدول العربية – تعلم مع زكريا_part1.wav\n",
            "Processing audio: عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية_part1.wav\n",
            "Processing audio: عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية_part2.wav\n",
            "Processing audio: تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار )_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار )_part1.wav\n",
            "Processing audio: تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار )_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار )_part2.wav\n",
            "Processing audio: الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر_part1.wav\n",
            "Processing audio: الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر_part2.wav\n",
            "Processing audio: Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال_part1.wav\n",
            "Processing audio: Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال_part2.wav\n",
            "Processing audio: تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين_part1.wav\n",
            "Processing audio: تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين_part2.wav\n",
            "Processing audio: ARABIC LANGUAGE ⧸ DAILY RUTINE. Listening for Beginners._part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/ARABIC LANGUAGE ⧸ DAILY RUTINE. Listening for Beginners._part1.wav\n",
            "Processing audio: 29.دورة تعليم القراءة في اللغة العربية Learn Arabic⧸alphabet⧸reading_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/29.دورة تعليم القراءة في اللغة العربية Learn Arabic⧸alphabet⧸reading_part1.wav\n",
            "Processing audio: Arabic Conversation for Beginners ｜ 70 Basic Arabic Phrases To Know_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/Arabic Conversation for Beginners ｜ 70 Basic Arabic Phrases To Know_part1.wav\n",
            "Processing audio: (75) نشرة أخبار اللغة العربية - 2019⧸11⧸01 م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(75) نشرة أخبار اللغة العربية - 2019⧸11⧸01 م_part1.wav\n",
            "Processing audio: (78) نشرة أخبار اللغة العربية - 2020⧸02⧸07 م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(78) نشرة أخبار اللغة العربية - 2020⧸02⧸07 م_part1.wav\n",
            "Processing audio: (63) نشرة أخبار اللغة العربية - 2019⧸03⧸21م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(63) نشرة أخبار اللغة العربية - 2019⧸03⧸21م_part1.wav\n",
            "Processing audio: (49) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸09⧸20م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(49) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸09⧸20م_part1.wav\n",
            "Processing audio: (51) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸10⧸05م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(51) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸10⧸05م_part1.wav\n",
            "Processing audio: (08) نشرة أخبار اللغة العربية الأسبوعية - 2017⧸12⧸01م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(08) نشرة أخبار اللغة العربية الأسبوعية - 2017⧸12⧸01م_part1.wav\n",
            "Processing audio: (81) نشرة أخبار اللغة العربية - 2021⧸12⧸31م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(81) نشرة أخبار اللغة العربية - 2021⧸12⧸31م_part1.wav\n",
            "Processing audio: (15) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸01⧸19م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(15) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸01⧸19م_part1.wav\n",
            "Processing audio: (17) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸02م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(17) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸02م_part1.wav\n",
            "Processing audio: نشرة أخبار الرياضة باللغة العربية 18⧸01⧸2013_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/نشرة أخبار الرياضة باللغة العربية 18⧸01⧸2013_part1.wav\n",
            "Processing audio: (19) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸16م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(19) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸16م_part1.wav\n",
            "Processing audio: (27) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸04⧸13م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(27) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸04⧸13م_part1.wav\n",
            "Processing audio: (33) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸25م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(33) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸25م_part1.wav\n",
            "Processing audio: (32) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸18م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(32) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸18م_part1.wav\n",
            "Processing audio: (37) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸29م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(37) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸29م_part1.wav\n",
            "Processing audio: (36) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸22م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(36) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸22م_part1.wav\n",
            "Processing audio: (38) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸06م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(38) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸06م_part1.wav\n",
            "Processing audio: (39) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸13م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(39) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸13م_part1.wav\n",
            "Processing audio: (41) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸27م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(41) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸27م_part1.wav\n",
            "Processing audio: (40) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸20م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(40) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸20م_part1.wav\n",
            "Processing audio: (43) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸10م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(43) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸10م_part1.wav\n",
            "Processing audio: (42) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸03م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(42) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸03م_part1.wav\n",
            "Processing audio: (44) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸17م_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/(44) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸17م_part1.wav\n",
            "Processing audio: كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست_part1.wav\n",
            "Processing audio: كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست_part2.wav\n",
            "Processing audio: اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل_part1.wav\n",
            "Processing audio: اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل_part2.wav\n",
            "Processing audio: كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس_part1.wav\n",
            "Processing audio: كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس_part2.wav\n",
            "Processing audio: كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري_part1.wav\n",
            "Processing audio: كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري_part2.wav\n",
            "Processing audio: علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي_part1.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي_part1.wav\n",
            "Processing audio: علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي_part2.mp3 -> /content/drive/MyDrive/SpeechDownloads/processed/audio/علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي_part2.wav\n",
            "Processing transcriptions...\n",
            "Transcriptions Dictionary Keys: ['تعليم العربية للناطقين بغيرها كيف تحسن لغتك العربية ｜ How to learn arabic.mp3_part1', 'طريق الفصاحة 1 (المستوى الأول) تعلم اللغة العربية للمبتدئين دراسة شاملة من العجمة إلى الفصاحة.mp3_part1', 'تعليم القراءة للاطفال ｜ تعلّم القراءة بحركة الفتح ｜ أسهل طريقة لتعليم القراءة للصغار مع زكريا.mp3_part1', '1- تعليم القراءة في اللغة العربية الدرس الأول  Arabic  alphabet and how to read the Arabic language.mp3_part1', 'هل تعلم؟ ｜ البلدان العربية (الحلقة ٩)  - أسئلة و أجوبة عن الدول العربية – تعلم مع زكريا.mp3_part1', 'عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية.mp3_part1', 'عشرة ｜ 10 أعمال فنية تتعلم منها اللغة العربية.mp3_part2', 'تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار ).mp3_part1', 'تعلم القراءة والكتابة مع أهم ست قواعد ( مهم للكبار والصغار ).mp3_part2', 'الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر.mp3_part1', 'الحروف الابجدية العربية مع الكلمات السهلة و الحركات تعلبم القراءة و الكتابة و للمبتدئين من الصفر.mp3_part2', 'Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال.mp3_part1', 'Arabic Learning for Babies & Toddlers - تعلم اللغة العربية للأطفال.mp3_part2', 'تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين.mp3_part1', 'تعلم أهم مبادئ النحو والإعراب في ٢٠ دقيقة -  للمبتدئين - مداخل العلوم الإسلامية - محمد بن شمس الدين.mp3_part2', 'ARABIC LANGUAGE ⧸ DAILY RUTINE. Listening for Beginners..mp3_part1', '29.دورة تعليم القراءة في اللغة العربية Learn Arabic⧸alphabet⧸reading.mp3_part1', 'Arabic Conversation for Beginners ｜ 70 Basic Arabic Phrases To Know.mp3_part1', '(75) نشرة أخبار اللغة العربية - 2019⧸11⧸01 م.mp3_part1', '(78) نشرة أخبار اللغة العربية - 2020⧸02⧸07 م.mp3_part1', '(63) نشرة أخبار اللغة العربية - 2019⧸03⧸21م.mp3_part1', '(49) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸09⧸20م.mp3_part1', '(51) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸10⧸05م.mp3_part1', '(08) نشرة أخبار اللغة العربية الأسبوعية - 2017⧸12⧸01م.mp3_part1', '(81) نشرة أخبار اللغة العربية - 2021⧸12⧸31م.mp3_part1', '(15) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸01⧸19م.mp3_part1', '(17) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸02م.mp3_part1', 'نشرة أخبار الرياضة باللغة العربية 18⧸01⧸2013.mp3_part1', '(19) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸02⧸16م.mp3_part1', '(27) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸04⧸13م.mp3_part1', '(33) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸25م.mp3_part1', '(32) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸5⧸18م.mp3_part1', '(37) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸29م.mp3_part1', '(36) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸06⧸22م.mp3_part1', '(38) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸06م.mp3_part1', '(39) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸13م.mp3_part1', '(41) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸27م.mp3_part1', '(40) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸07⧸20م.mp3_part1', '(43) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸10م.mp3_part1', '(42) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸03م.mp3_part1', '(44) نشرة أخبار اللغة العربية الأسبوعية - 2018⧸08⧸17م.mp3_part1', 'كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست.mp3_part1', 'كتاب مسموع قوة التفكير الإيجابي ملخص كتاب نورمان فينيست.mp3_part2', 'اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل.mp3_part1', 'اربع قصص قصيرة لـ تشيخوف ｜ بصوت إسلام عادل.mp3_part2', 'كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس.mp3_part1', 'كيف تكشف وتقاوم التلاعب النفسي - ملخص كتاب كيف تقرأ الناس.mp3_part2', 'كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري.mp3_part1', 'كتب صوتية مسموعة - 30 يوما مع الله - فيصل أحمد بخاري.mp3_part2', 'علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي.mp3_part1', 'علم البسطاء：  الموسم السابع： الحلقة 1 ｜ ناشونال جيوغرافيك أبوظبي.mp3_part2']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def preprocess_audio(audio_dir, output_dir, sample_rate=16000):\n",
        "    \"\"\"Convert audio files to WAV format with the required sample rate.\"\"\"\n",
        "    processed_audio_dir = os.path.join(output_dir, \"audio\")\n",
        "    os.makedirs(processed_audio_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in os.listdir(audio_dir):\n",
        "        if file_name.endswith(\".mp3\"):\n",
        "            input_path = os.path.join(audio_dir, file_name)\n",
        "            output_path = os.path.join(processed_audio_dir, f\"{os.path.splitext(file_name)[0]}.wav\")\n",
        "\n",
        "            # Convert to WAV with the desired sample rate\n",
        "            print(f\"Processing audio: {file_name} -> {output_path}\")\n",
        "            audio = AudioSegment.from_mp3(input_path)\n",
        "            audio = audio.set_frame_rate(sample_rate)\n",
        "            audio.export(output_path, format=\"wav\")\n",
        "\n",
        "    return processed_audio_dir\n",
        "\n",
        "# # Preprocess text transcriptions\n",
        "# def preprocess_text(transcriptions_dir):\n",
        "#     \"\"\"Normalize and clean text transcriptions.\"\"\"\n",
        "#     transcriptions = {}\n",
        "#     for file_name in os.listdir(transcriptions_dir):\n",
        "#         if file_name.endswith(\".txt\"):\n",
        "#             input_path = os.path.join(transcriptions_dir, file_name)\n",
        "#             with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#                 text = f.read().strip()\n",
        "#                 # Normalize the filename for consistent matching\n",
        "#                 normalized_key = normalize_filename(os.path.splitext(file_name)[0])\n",
        "#                 transcriptions[normalized_key] = text\n",
        "#     return transcriptions\n",
        "\n",
        "\n",
        "def preprocess_text(transcriptions_dir, output_dir):\n",
        "    \"\"\"Normalize and clean text transcriptions.\"\"\"\n",
        "    transcriptions = {}\n",
        "    for file_name in os.listdir(transcriptions_dir):\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            input_path = os.path.join(transcriptions_dir, file_name)\n",
        "            with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                text = f.read().strip()\n",
        "                # Normalize the filename for consistent matching\n",
        "                normalized_key = os.path.splitext(file_name)[0]\n",
        "                transcriptions[normalized_key] = text\n",
        "\n",
        "                # Save cleaned transcription in output directory\n",
        "                output_path = os.path.join(output_dir, f\"{normalized_key}.txt\")\n",
        "                with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
        "                    output_file.write(text)\n",
        "\n",
        "    return transcriptions\n",
        "\n",
        "# Main Script\n",
        "\n",
        "# Preprocess\n",
        "print(\"Processing audio files...\")\n",
        "processed_audio_dir = preprocess_audio(audio_dir, output_dir)\n",
        "\n",
        "print(\"Processing transcriptions...\")\n",
        "transcriptions = preprocess_text(\"/content/drive/MyDrive/SpeechDownloads/transcriptions\", \"/content/drive/MyDrive/SpeechDownloads/processed/transcriptions\")\n",
        "print(f\"Transcriptions Dictionary Keys: {list(transcriptions.keys())}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "csftJbmjpAfG"
      },
      "outputs": [],
      "source": [
        "# # Normalize filenames by removing special characters, spaces, and parts like 'part1'\n",
        "# def normalize_filename(filename):\n",
        "#     # Remove 'mp3', 'part1', 'part2' and similar parts, and any special characters or spaces\n",
        "#     filename = re.sub(r'\\.mp3$', '', filename)  # Remove 'mp3' extension if any\n",
        "#     filename = re.sub(r'\\.wav$', '', filename)  # Remove 'wav' extension if any\n",
        "#     filename = re.sub(r'\\.txt$', '', filename)  # Remove 'txt' extension if any\n",
        "#     filename = re.sub(r'\\.[^.]*$', '', filename)  # Remove any file extension after the first dot\n",
        "#     filename = re.sub(r'[^\\w\\s]', '', filename)  # Remove special characters\n",
        "#     return re.sub(r'\\s+', '_', filename).strip()  # Replace spaces with underscores and clean up\n",
        "\n",
        "\n",
        "#     # Create metadata linking audio and text\n",
        "# def create_metadata(audio_dir, transcriptions, output_path):\n",
        "#     \"\"\"Create a metadata CSV file linking audio and text.\"\"\"\n",
        "#     metadata = []\n",
        "#     for file_name in os.listdir(audio_dir):\n",
        "#         if file_name.endswith(\".wav\"):\n",
        "#             base_name = os.path.splitext(file_name)[0]\n",
        "#             # Normalize filenames for matching\n",
        "#             audio_key = normalize_filename(base_name)\n",
        "#             text = transcriptions.get(audio_key, \"\")\n",
        "#             if text:\n",
        "#                 metadata.append([os.path.join(\"audio\", file_name), text])\n",
        "#                 print(f\"Metadata entry added: {file_name} -> {text[:50]}...\")\n",
        "#             else:\n",
        "#                 print(f\"No matching transcription found for: {file_name}\")\n",
        "#     # Save metadata as CSV\n",
        "#     if metadata:\n",
        "#         metadata_df = pd.DataFrame(metadata, columns=[\"audio_path\", \"text\"])\n",
        "#         metadata_df.to_csv(output_path, sep=\"|\", header=False, index=False)\n",
        "#         print(f\"Metadata saved: {output_path} with {len(metadata)} entries.\")\n",
        "#     else:\n",
        "#         print(\"No metadata entries were created. Check your inputs!\")\n",
        "\n",
        "\n",
        "# print(\"Creating metadata...\")\n",
        "# create_metadata(\"/content/drive/MyDrive/SpeechDownloads/processed/audio\", transcriptions, \"/content/drive/MyDrive/SpeechDownloads/processed/metadata.csv\")\n",
        "\n",
        "# print(\"Preprocessing complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ssXobKRgcRKl",
        "outputId": "96fcc0f3-3cf5-4095-dbfe-b4ae48ab6877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers librosa\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P80sLj_1goPT",
        "outputId": "203be0d4-1ba3-4514-decb-d1af9c10f0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Downloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.46.3\n",
            "    Uninstalling transformers-4.46.3:\n",
            "      Successfully uninstalled transformers-4.46.3\n",
            "Successfully installed tokenizers-0.21.0 transformers-4.47.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers torch librosa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1ZZPdRkXrR_W"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import librosa\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, Model\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# # Step 1: Define Tacotron 2 Architecture (Simplified for Training)\n",
        "# class Tacotron2(Model):\n",
        "#     def __init__(self, input_dim, embedding_dim, output_dim):\n",
        "#         super(Tacotron2, self).__init__()\n",
        "#         self.encoder = layers.LSTM(embedding_dim, return_sequences=True)\n",
        "#         self.decoder = layers.LSTM(output_dim, return_sequences=True)\n",
        "#         self.dense = layers.Dense(output_dim)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         x = self.encoder(x)\n",
        "#         x = self.decoder(x)\n",
        "#         x = self.dense(x)\n",
        "#         return x\n",
        "\n",
        "# # Step 2: Preprocess Audio Files (Convert to Mel Spectrograms)\n",
        "# def preprocess_audio(audio_path, sr=22050, n_fft=1024, hop_length=256, n_mels=80):\n",
        "#     # Load audio\n",
        "#     audio, _ = librosa.load(audio_path, sr=sr)\n",
        "#     # Normalize audio\n",
        "#     audio = librosa.util.normalize(audio)\n",
        "#     # Convert to Mel spectrogram\n",
        "#     mel_spectrogram = librosa.feature.melspectrogram(\n",
        "#         y=audio, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels\n",
        "#     )\n",
        "#     mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "#     return mel_spectrogram.T  # Transpose for time-major format\n",
        "\n",
        "# # Step 3: Load Data (Audio and Transcriptions)\n",
        "# def load_data(audio_dir, transcription_dir):\n",
        "#     audio_files = sorted(os.listdir(audio_dir))\n",
        "#     transcription_files = sorted(os.listdir(transcription_dir))\n",
        "\n",
        "#     audio_data, transcription_data = [], []\n",
        "\n",
        "#     for audio_file, transcription_file in zip(audio_files, transcription_files):\n",
        "#         audio_path = os.path.join(audio_dir, audio_file)\n",
        "#         transcription_path = os.path.join(transcription_dir, transcription_file)\n",
        "\n",
        "#         # Preprocess audio\n",
        "#         mel_spectrogram = preprocess_audio(audio_path)\n",
        "#         audio_data.append(mel_spectrogram)\n",
        "\n",
        "#         # Load transcription\n",
        "#         with open(transcription_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#             transcription = f.read().strip()\n",
        "#         transcription_data.append(transcription)\n",
        "\n",
        "#     return audio_data, transcription_data\n",
        "\n",
        "# # Step 4: Tokenize Transcriptions\n",
        "# def tokenize_transcriptions(transcriptions):\n",
        "#     # Build vocabulary\n",
        "#     vocab = sorted(set(\"\".join(transcriptions)))\n",
        "#     char_to_id = {char: idx for idx, char in enumerate(vocab)}\n",
        "#     id_to_char = {idx: char for char, idx in char_to_id.items()}\n",
        "\n",
        "#     # Convert to integer sequences\n",
        "#     tokenized = [[char_to_id[char] for char in text] for text in transcriptions]\n",
        "\n",
        "#     return tokenized, char_to_id, id_to_char\n",
        "\n",
        "# # Step 5: Train Tacotron 2\n",
        "# def train_tacotron2(audio_dir, transcription_dir, output_dim=80, batch_size=32, epochs=5):\n",
        "#     # Load data\n",
        "#     audio_data, transcription_data = load_data(audio_dir, transcription_dir)\n",
        "\n",
        "#     # Tokenize transcriptions\n",
        "#     tokenized_transcriptions, char_to_id, _ = tokenize_transcriptions(transcription_data)\n",
        "\n",
        "#     # Pad sequences\n",
        "#     audio_data = tf.keras.preprocessing.sequence.pad_sequences(audio_data, padding=\"post\", dtype=\"float32\")\n",
        "#     tokenized_transcriptions = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "#         tokenized_transcriptions, padding=\"post\"\n",
        "#     )\n",
        "\n",
        "#     # Split into train/test sets\n",
        "#     X_train, X_val, y_train, y_val = train_test_split(\n",
        "#         audio_data, tokenized_transcriptions, test_size=0.1, random_state=42\n",
        "#     )\n",
        "\n",
        "#     # Define model\n",
        "#     input_dim = audio_data.shape[-1]\n",
        "#     model = Tacotron2(input_dim=input_dim, embedding_dim=256, output_dim=output_dim)\n",
        "#     model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(\n",
        "#         X_train,\n",
        "#         y_train,\n",
        "#         validation_data=(X_val, y_val),\n",
        "#         batch_size=batch_size,\n",
        "#         epochs=epochs,\n",
        "#     )\n",
        "\n",
        "#     # Save the model\n",
        "#     model.save(\"tacotron2_model.h5\")\n",
        "#     print(\"Model saved as tacotron2_model.h5\")\n",
        "\n",
        "# # Step 6: Run the Training Workflow\n",
        "# audio_dir = \"/content/drive/MyDrive/SpeechDownloads/processed/audio\"\n",
        "# transcription_dir = \"/content/drive/MyDrive/SpeechDownloads/transcriptions\"\n",
        "# train_tacotron2(audio_dir, transcription_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JJa8C-vVsMzD"
      },
      "outputs": [],
      "source": [
        "def pad_and_align(audio_data, tokenized_transcriptions):\n",
        "    max_audio_length = max([a.shape[0] for a in audio_data])  # Longest Mel spectrogram\n",
        "    max_text_length = max([len(t) for t in tokenized_transcriptions])  # Longest transcription\n",
        "\n",
        "    # Pad Mel spectrograms (to max_audio_length)\n",
        "    padded_audio_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        audio_data, maxlen=max_audio_length, padding=\"post\", dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # Pad tokenized transcriptions (to max_text_length)\n",
        "    padded_transcriptions = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenized_transcriptions, maxlen=max_text_length, padding=\"post\"\n",
        "    )\n",
        "\n",
        "    return padded_audio_data, padded_transcriptions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-p_IEniAVo7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import librosa\n",
        "\n",
        "# Function to load audio data\n",
        "def load_audio_files(audio_dir):\n",
        "    audio_files = []\n",
        "    for filename in os.listdir(audio_dir):\n",
        "        if filename.endswith(\".wav\"):  # Adjust if you're using a different format\n",
        "            file_path = os.path.join(audio_dir, filename)\n",
        "            audio, _ = librosa.load(file_path, sr=22050)  # Change the sample rate if needed\n",
        "            audio_files.append(audio)\n",
        "    return audio_files\n",
        "\n",
        "# Function to load transcription data\n",
        "def load_transcriptions(transcription_dir):\n",
        "    transcription_files = []\n",
        "    for filename in os.listdir(transcription_dir):\n",
        "        if filename.endswith(\".txt\"):  # Adjust if you're using a different format\n",
        "            file_path = os.path.join(transcription_dir, filename)\n",
        "            with open(file_path, 'r') as file:\n",
        "                transcription = file.read().strip()  # Read the transcription\n",
        "                transcription_files.append(transcription)\n",
        "    return transcription_files\n",
        "\n",
        "# Function to load both audio and transcription data\n",
        "def load_data(audio_dir, transcription_dir):\n",
        "    audio_data = load_audio_files(audio_dir)\n",
        "    transcription_data = load_transcriptions(transcription_dir)\n",
        "    return audio_data, transcription_data\n",
        "\n",
        "# Tokenize transcriptions (you can adjust the tokenizer to fit your use case)\n",
        "def tokenize_transcriptions(transcription_data):\n",
        "    # Simple tokenizer that maps each character to an integer ID\n",
        "    char_to_id = {char: idx for idx, char in enumerate(\"abcdefghijklmnopqrstuvwxyz \")}\n",
        "    tokenized_transcriptions = []\n",
        "\n",
        "    for transcription in transcription_data:\n",
        "        tokenized = [char_to_id[char] for char in transcription.lower() if char in char_to_id]\n",
        "        tokenized_transcriptions.append(tokenized)\n",
        "\n",
        "    return tokenized_transcriptions, char_to_id\n",
        "\n",
        "# Function to pad and align audio and transcription data\n",
        "def pad_and_align(audio_data, tokenized_transcriptions, max_audio_len=None, max_trans_len=None):\n",
        "    # Get maximum lengths if not provided\n",
        "    if max_audio_len is None:\n",
        "        max_audio_len = max([len(audio) for audio in audio_data])\n",
        "    if max_trans_len is None:\n",
        "        max_trans_len = max([len(t) for t in tokenized_transcriptions])\n",
        "\n",
        "    # Padding audio data\n",
        "    padded_audio_data = [audio.tolist() + [0] * (max_audio_len - len(audio)) if len(audio) < max_audio_len else audio.tolist()[:max_audio_len] for audio in audio_data]\n",
        "\n",
        "    # Padding transcriptions\n",
        "    padded_tokenized_transcriptions = [transcription + [0] * (max_trans_len - len(transcription)) if len(transcription) < max_trans_len else transcription[:max_trans_len] for transcription in tokenized_transcriptions]\n",
        "\n",
        "    return padded_audio_data, padded_tokenized_transcriptions\n",
        "\n",
        "# Define the Tacotron2 model (Simplified version for illustration)\n",
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, output_dim):\n",
        "        super(Tacotron2, self).__init__()\n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(256, embedding_dim)  # Assuming 256 character tokens\n",
        "        self.lstm = nn.LSTM(input_dim, embedding_dim, batch_first=True)\n",
        "        self.linear = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "# Training function for Tacotron2\n",
        "def traintacotron2(audio_dir, transcription_dir, output_dim=80, batch_size=32, epochs=5):\n",
        "    # Load data\n",
        "    audio_data, transcription_data = load_data(audio_dir, transcription_dir)\n",
        "\n",
        "    # Tokenize transcriptions\n",
        "    tokenized_transcriptions, char_to_id = tokenize_transcriptions(transcription_data)\n",
        "\n",
        "    # Pad and align data\n",
        "    audio_data, tokenized_transcriptions = pad_and_align(audio_data, tokenized_transcriptions)\n",
        "\n",
        "    # Split into train/test sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        audio_data, tokenized_transcriptions, test_size=0.1, random_state=42\n",
        "    )\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Assuming target is class indices\n",
        "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader for efficient batching\n",
        "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_data = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Define model\n",
        "    input_dim = X_train_tensor.shape[-1]  # Feature size of Mel spectrograms (or audio data)\n",
        "    model = Tacotron2(input_dim=input_dim, embedding_dim=256, output_dim=output_dim)\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Enable mixed precision training\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Loop through batches\n",
        "        for inputs, targets in train_loader:\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs.view(-1, output_dim), targets.view(-1))\n",
        "\n",
        "            # Scale the loss for mixed precision\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            # Accumulate loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print loss after every epoch\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss = 0.0\n",
        "            for inputs, targets in val_loader:\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs.view(-1, output_dim), targets.view(-1))\n",
        "                val_loss += loss.item()\n",
        "            print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        torch.save(model.state_dict(), f\"tacotron2_epoch{epoch+1}.pth\")\n",
        "        print(f\"Model saved as tacotron2_epoch{epoch+1}.pth\")\n",
        "\n",
        "# Example usage:\n",
        "traintacotron2('/content/drive/MyDrive/SpeechDownloads/processed/audio', '/content/drive/MyDrive/SpeechDownloads/processed/transcriptions', epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lKbVTpQBD7iR"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "# import torchaudio\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Load Wav2Vec2 Processor and Model\n",
        "# model_name = \"facebook/wav2vec2-base\"\n",
        "# processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "# model = Wav2Vec2Model.from_pretrained(model_name)\n",
        "\n",
        "# # Move model to GPU if available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # Function to load and preprocess audio\n",
        "# def preprocess_audio_torch(file_path, sampling_rate=16000):\n",
        "#     waveform, sr = torchaudio.load(file_path, normalize=True)\n",
        "#     if sr != sampling_rate:\n",
        "#         resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sampling_rate)\n",
        "#         waveform = resampler(waveform)\n",
        "#     return waveform.squeeze(0).numpy()\n",
        "\n",
        "# # Function to extract embeddings\n",
        "# def extract_embeddings(audio, processor, model):\n",
        "#     inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "#     inputs = inputs.to(device)  # Move inputs to GPU\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**inputs)\n",
        "#     embeddings = outputs.last_hidden_state\n",
        "#     return embeddings\n",
        "\n",
        "# # Directory containing processed audio files\n",
        "# audio_dir = \"/content/drive/MyDrive/SpeechDownloads/processed/audio\"\n",
        "\n",
        "# # Output directory for embeddings\n",
        "# output_dir = \"/content/drive/MyDrive/SpeechDownloads/processed/embeddings\"\n",
        "# os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# # Process all audio files in the directory\n",
        "# for audio_file in tqdm(os.listdir(audio_dir)):\n",
        "#     if audio_file.endswith(\".wav\"):\n",
        "#         try:\n",
        "#             file_path = os.path.join(audio_dir, audio_file)\n",
        "#             print(f\"Processing file: {file_path}\")\n",
        "\n",
        "#             # Preprocess and extract embeddings\n",
        "#             audio = preprocess_audio_torch(file_path)\n",
        "#             embeddings = extract_embeddings(audio, processor, model)\n",
        "\n",
        "#             # Convert embeddings to numpy\n",
        "#             embeddings_numpy = embeddings.squeeze(0).cpu().numpy()  # Move to CPU if on GPU\n",
        "\n",
        "#             # Save embeddings as .npy file\n",
        "#             output_path = os.path.join(output_dir, f\"{os.path.splitext(audio_file)[0]}_embeddings.npy\")\n",
        "#             np.save(output_path, embeddings_numpy)\n",
        "#             print(f\"Saved embeddings to {output_path}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error processing {audio_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "35QMz0JJy-2J"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Read the transcriptions from the text file\n",
        "# with open(\"/content/drive/MyDrive/SpeechDownloads/transcriptions/15 شيء لا يجب أن يراه البشر !!!.mp3_part1.txt\", \"r\") as f:\n",
        "#     transcriptions = f.readlines()\n",
        "\n",
        "# # Step 2: Preprocess the transcriptions (e.g., convert to list of tokens or character IDs)\n",
        "# # For simplicity, we'll tokenize by splitting by spaces (character-level encoding can also be done)\n",
        "# # For phoneme-based transcriptions, additional preprocessing might be needed.\n",
        "\n",
        "# # Here, we're just converting each transcription into a list of characters\n",
        "# processed_transcriptions = [list(transcription.strip()) for transcription in transcriptions]\n",
        "\n",
        "# # Optionally, you can convert characters to integer IDs using a vocabulary\n",
        "# # For example, creating a simple vocabulary of characters:\n",
        "# vocab = set(''.join(transcriptions))  # Get unique characters\n",
        "# char_to_id = {char: idx for idx, char in enumerate(vocab)}  # Map characters to IDs\n",
        "\n",
        "# # Convert characters to integer IDs\n",
        "# transcriptions_as_ids = [[char_to_id[char] for char in transcription] for transcription in processed_transcriptions]\n",
        "\n",
        "# # Step 3: Save the transcriptions as a .npy file\n",
        "# np.save(\"/content/drive/MyDrive/SpeechDownloads/transcriptions.npy\", transcriptions_as_ids)\n",
        "\n",
        "# print(\"Transcriptions have been saved as transcriptions.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jVX8c-dg4mfR"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # Step 1: Read the transcriptions from the text file\n",
        "# with open(\"/content/drive/MyDrive/SpeechDownloads/transcriptions/15 شيء لا يجب أن يراه البشر !!!.mp3_part1.txt\", \"r\") as f:\n",
        "#     transcriptions = f.readlines()\n",
        "\n",
        "# # Step 2: Preprocess the transcriptions\n",
        "# # Tokenizing by splitting by spaces (character-level encoding can also be done)\n",
        "# processed_transcriptions = [list(transcription.strip()) for transcription in transcriptions]\n",
        "\n",
        "# # Create a simple vocabulary of characters from the transcriptions\n",
        "# vocab = set(''.join(transcriptions))  # Get unique characters\n",
        "# char_to_id = {char: idx for idx, char in enumerate(vocab)}  # Map characters to IDs\n",
        "\n",
        "# # Convert characters to integer IDs\n",
        "# transcriptions_as_ids = [[char_to_id[char] for char in transcription] for transcription in processed_transcriptions]\n",
        "\n",
        "# # Step 3: Ensure the number of transcriptions matches the number of embeddings\n",
        "# # Here, we need to match the size to 33750\n",
        "# desired_size = 33750\n",
        "\n",
        "# # If there are more transcriptions than the desired size, slice them\n",
        "# if len(transcriptions_as_ids) > desired_size:\n",
        "#     transcriptions_as_ids = transcriptions_as_ids[:desired_size]\n",
        "\n",
        "# # If there are fewer transcriptions than the desired size, repeat or pad them\n",
        "# elif len(transcriptions_as_ids) < desired_size:\n",
        "#     # Repeat the transcriptions to reach the desired size (or pad with empty sequences)\n",
        "#     while len(transcriptions_as_ids) < desired_size:\n",
        "#         transcriptions_as_ids.extend(transcriptions_as_ids[:desired_size - len(transcriptions_as_ids)])\n",
        "\n",
        "# # Step 4: Save the processed transcriptions as a .npy file\n",
        "# np.save(\"/content/drive/MyDrive/SpeechDownloads/transcriptions.npy\", transcriptions_as_ids)\n",
        "\n",
        "# print(\"Transcriptions have been saved as transcriptions.npy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "73gz0ttJdvGS"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, Model\n",
        "\n",
        "# # Define a simple Tacotron 2-like architecture (simplified for example)\n",
        "# class Tacotron2(Model):\n",
        "#     def __init__(self, embedding_dim, output_dim, max_length):\n",
        "#         super(Tacotron2, self).__init__()\n",
        "#         self.encoder = layers.LSTM(embedding_dim, return_sequences=True)\n",
        "#         self.decoder = layers.LSTM(output_dim, return_sequences=True)\n",
        "#         self.output_layer = layers.Dense(output_dim)  # For generating output features (e.g., mel spectrogram)\n",
        "#         self.max_length = max_length  # Maximum length of transcription\n",
        "\n",
        "#     def call(self, x):\n",
        "#         x = self.encoder(x)  # Encoder outputs sequence\n",
        "#         x = self.decoder(x)  # Decoder generates output sequence\n",
        "#         x = self.output_layer(x)  # Output features at each timestep\n",
        "#         return x\n",
        "\n",
        "# # Prepare data for training (Wave2Vec embeddings and transcriptions)\n",
        "# wave2vec_embeddings = np.load(\"/content/drive/MyDrive/SpeechDownloads/processed/embeddings/15 شيء لا يجب أن يراه البشر !!!_part1_embeddings.npy\")\n",
        "# transcriptions = np.load(\"/content/drive/MyDrive/SpeechDownloads/transcriptions.npy\")\n",
        "\n",
        "# # Step 1: Reshape the embeddings to include the time dimension\n",
        "# wave2vec_embeddings = np.expand_dims(wave2vec_embeddings, axis=1)  # Add time dimension\n",
        "\n",
        "# # Step 2: Check the shape of wave2vec_embeddings after reshaping\n",
        "# print(f\"Shape of wave2vec_embeddings after reshaping: {wave2vec_embeddings.shape}\")\n",
        "\n",
        "# # Step 3: Adjust the shape of transcriptions to match the output sequence length\n",
        "# # Make sure the transcriptions have the same sequence length as the output of the model\n",
        "# max_length = 7738  # Set this to the expected length of your transcriptions\n",
        "# transcriptions = np.pad(transcriptions, ((0, 0), (0, max_length - transcriptions.shape[1])), 'constant')\n",
        "\n",
        "# # Define the model\n",
        "# model = Tacotron2(embedding_dim=7738, output_dim=80, max_length=max_length)\n",
        "# model.compile(optimizer=\"adam\", loss=\"mse\")\n",
        "\n",
        "# # Step 4: Train the model\n",
        "# model.fit(wave2vec_embeddings, transcriptions, epochs=5)\n",
        "\n",
        "# # Save the model weights\n",
        "# model.save_weights(\"tacotron2_weights.h5\")\n",
        "# print(\"Model weights saved as tacotron2_weights.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0l3MtQNPX-q"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-g-vbOy2YJu",
        "outputId": "f2930ede-03b1-476a-d8b4-c9c77894eb1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.5 rapidfuzz-3.10.1\n"
          ]
        }
      ],
      "source": [
        "# pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df1QRu05PEAg"
      },
      "outputs": [],
      "source": [
        "# import jiwer\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# # Assuming the model is loaded and the data is available\n",
        "# def evaluate_model(model, dataloader, device):\n",
        "#     model.eval()\n",
        "#     predictions = []\n",
        "#     ground_truth = []\n",
        "\n",
        "#     # Iterate over the validation set\n",
        "#     for inputs, targets in dataloader:\n",
        "#         inputs = inputs.squeeze(1).to(device)  # Remove channel dimension for the model\n",
        "#         targets = targets.to(device)\n",
        "\n",
        "#         # Perform inference\n",
        "#         with torch.no_grad():\n",
        "#             outputs = model(inputs)\n",
        "\n",
        "#         # Convert output probabilities to text indices\n",
        "#         predicted_transcriptions = torch.argmax(outputs, dim=2)\n",
        "\n",
        "#         # Convert indices to text (decode predicted transcriptions)\n",
        "#         decoded_preds = decode(predicted_transcriptions)\n",
        "#         decoded_targets = decode(targets)\n",
        "\n",
        "#         predictions.extend(decoded_preds)\n",
        "#         ground_truth.extend(decoded_targets)\n",
        "\n",
        "#     # Calculate Word Error Rate (WER)\n",
        "#     wer_score = jiwer.wer(ground_truth, predictions)\n",
        "#     return wer_score\n",
        "\n",
        "# # Assuming the decode function converts indices to text based on your tokenization method\n",
        "# def decode(sequence):\n",
        "#     # Implement your method to decode indices to text\n",
        "#     pass\n",
        "\n",
        "# # Example usage:\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# wer_score = evaluate_model(model, val_loader, device)\n",
        "# print(f\"Word Error Rate (WER): {wer_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePKmBiSlPbM6"
      },
      "source": [
        "**Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eXwfjgGPJUc",
        "outputId": "8953a02d-afbe-4a50-fc83-ac2faa694d60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pesq\n",
            "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pesq\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp310-cp310-linux_x86_64.whl size=262948 sha256=9669f09a68db77cc393586e8ec60af059185dd5d80d5562e9dd9a2534fc44367\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4e/2c/251524370c0fdd659e99639a0fbd0ca5a782c3aafcd456b28d\n",
            "Successfully built pesq\n",
            "Installing collected packages: pesq\n",
            "Successfully installed pesq-0.0.4\n"
          ]
        }
      ],
      "source": [
        "# pip install pesq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdcdOgZ3PK5l"
      },
      "outputs": [],
      "source": [
        "# from pesq import pesq\n",
        "# import librosa\n",
        "\n",
        "# def compute_pesq(reference_audio, generated_audio, sr=22050):\n",
        "#     # Load the reference and generated audios\n",
        "#     reference, _ = librosa.load(reference_audio, sr=sr)\n",
        "#     generated, _ = librosa.load(generated_audio, sr=sr)\n",
        "\n",
        "#     # Compute PESQ score\n",
        "#     pesq_score = pesq(reference, generated, sr)\n",
        "#     return pesq_score\n",
        "\n",
        "# # Example usage\n",
        "# reference_audio_path = \"reference.wav\"\n",
        "# generated_audio_path = \"generated.wav\"\n",
        "# pesq_score = compute_pesq(reference_audio_path, generated_audio_path)\n",
        "# print(f\"PESQ Score: {pesq_score}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40TNVy0cPP3f"
      },
      "source": [
        "**AB Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpdznZ9gPNnx"
      },
      "outputs": [],
      "source": [
        "# from scipy import stats\n",
        "\n",
        "# def ab_test(model_a, model_b, val_loader, device):\n",
        "#     # Evaluate both models\n",
        "#     wer_model_a = evaluate_model(model_a, val_loader, device)\n",
        "#     wer_model_b = evaluate_model(model_b, val_loader, device)\n",
        "\n",
        "#     print(f\"Model A WER: {wer_model_a}\")\n",
        "#     print(f\"Model B WER: {wer_model_b}\")\n",
        "\n",
        "#     # Perform t-test to see if there's a significant difference\n",
        "#     # In practice, you'd collect multiple results, not just one.\n",
        "#     t_stat, p_value = stats.ttest_ind(wer_model_a, wer_model_b)\n",
        "\n",
        "#     if p_value < 0.05:\n",
        "#         print(\"The difference between models is statistically significant!\")\n",
        "#     else:\n",
        "#         print(\"No significant difference between models.\")\n",
        "\n",
        "# # Example usage:\n",
        "# ab_test(model_a, model_b, val_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
